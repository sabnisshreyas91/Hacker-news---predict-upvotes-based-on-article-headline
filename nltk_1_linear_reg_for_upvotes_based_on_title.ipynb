{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported necessary libraries\n",
      "\n",
      "'stories' dataframe has 20000 rows from the stories.csv file\n",
      "\n",
      "first 5 rows for inspection\n",
      "\n",
      "        id                created_at  created_at_i        author  points  \\\n",
      "0  9079978  2015-02-20T11:29:58.000Z    1424431798    Immortalin       2   \n",
      "1  9079983  2015-02-20T11:34:22.000Z    1424432062     Rutger24s       1   \n",
      "2  9079986  2015-02-20T11:35:32.000Z    1424432132  AndrewDucker       3   \n",
      "3  9079988  2015-02-20T11:36:18.000Z    1424432178     davidiach       1   \n",
      "4  9080000  2015-02-20T11:41:06.000Z    1424432466       CiaranR       1   \n",
      "\n",
      "          url_hostname  num_comments  \\\n",
      "0                  NaN             0   \n",
      "1  startupjuncture.com             0   \n",
      "2   blog.erratasec.com             0   \n",
      "3          twitter.com             0   \n",
      "4  phpconference.co.uk             0   \n",
      "\n",
      "                                               title  \n",
      "0       Ask HN: Simple SaaS as first Golang web app?  \n",
      "1   24sessions: live business advice over video-chat  \n",
      "2                            Some notes on SuperFish  \n",
      "3    Apple Watch models could contain 29.16g of gold  \n",
      "4  PHP UK Conference Diversity Scholarship Programme  \n",
      "\n",
      "data sense checks...\n",
      "story id column unique\n",
      "\n",
      "missing values present\n",
      "columns and number of missing values:\n",
      "id                 0\n",
      "created_at         0\n",
      "created_at_i       0\n",
      "author             0\n",
      "points             0\n",
      "url_hostname    1092\n",
      "num_comments       0\n",
      "title              0\n",
      "dtype: int64\n",
      "\n",
      "dropped all rows where title was NULL\n",
      "ignore rows where url_hostname is NULL\n",
      "\n",
      "converted all title text to lower case and removed all punctuations\n",
      "\n",
      "5 rows for inspection\n",
      "\n",
      "        id                created_at  created_at_i        author  points  \\\n",
      "0  9079978  2015-02-20T11:29:58.000Z    1424431798    Immortalin       2   \n",
      "1  9079983  2015-02-20T11:34:22.000Z    1424432062     Rutger24s       1   \n",
      "2  9079986  2015-02-20T11:35:32.000Z    1424432132  AndrewDucker       3   \n",
      "3  9079988  2015-02-20T11:36:18.000Z    1424432178     davidiach       1   \n",
      "4  9080000  2015-02-20T11:41:06.000Z    1424432466       CiaranR       1   \n",
      "\n",
      "          url_hostname  num_comments  \\\n",
      "0                  NaN             0   \n",
      "1  startupjuncture.com             0   \n",
      "2   blog.erratasec.com             0   \n",
      "3          twitter.com             0   \n",
      "4  phpconference.co.uk             0   \n",
      "\n",
      "                                               title  \n",
      "0         ask hn simple saas as first golang web app  \n",
      "1     24sessions live business advice over videochat  \n",
      "2                            some notes on superfish  \n",
      "3     apple watch models could contain 2916g of gold  \n",
      "4  php uk conference diversity scholarship programme  \n",
      "\n",
      "tokenized title into individual words in a new column - tokenized title\n",
      "\n",
      "compiled series of distinct words and their counts in series word_counts\n",
      "\n",
      "first 5 rows of word_counts \n",
      "\n",
      "ask        774\n",
      "hn        1558\n",
      "simple     121\n",
      "saas        41\n",
      "as         287\n",
      "Name: word_counts, dtype: int64\n",
      "removed words that occur only once\n",
      "\n",
      "removed stop words \\(default stopwords from nltk\\) from word_counts\n",
      "\n",
      "removed words that repeat less than 20 times\n",
      "\n",
      "we now have 1015 words to use as features\n",
      "created dataframe 'counts_df' with the {} words/features as columns to implement bag of words model\n",
      "\n",
      "first 5 rows of the 'counts_df' dataframe for inspection\n",
      "split 'counts_df' into 80% train and 20% test sets\n",
      "\n",
      "trained linear regression model\n",
      "\n",
      "predicted upvotes for the test set\n",
      "\n",
      "Root mean error is:\n",
      "45.340088244947125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "print('imported necessary libraries')\n",
    "print('')\n",
    "\n",
    "\n",
    "stories=pd.read_csv('G:\\\\stories.csv',names=['id', 'created_at', 'created_at_i', 'author', 'points', 'url_hostname', 'num_comments', 'title'])\n",
    "row_subset=20000\n",
    "stories=stories.loc[0:row_subset,:]\n",
    "print('\\'stories\\' dataframe has {} rows from the stories.csv file'.format(row_subset))\n",
    "print('')\n",
    "print('first 5 rows for inspection')\n",
    "print('')\n",
    "print(stories.head())\n",
    "print('')\n",
    "\n",
    "\n",
    "print('data sense checks...')\n",
    "if(stories['id'].nunique()!=stories.shape[0]):\n",
    "    print('story id column not unique')\n",
    "else:\n",
    "    print('story id column unique')\n",
    "print('')\n",
    "if(stories.count().min()!=stories.shape[0]):\n",
    "    print('missing values present')\n",
    "    print('columns and number of missing values:')\n",
    "    print(stories.shape[0]-stories.count())\n",
    "    print('')\n",
    "    stories=stories.dropna(subset=['title'])\n",
    "    print('dropped all rows where title was NULL')\n",
    "    print('ignore rows where url_hostname is NULL')\n",
    "else:\n",
    "    print('no missing values present')\n",
    "print('')\n",
    "\n",
    "\n",
    "stories.loc[:,'title']=stories['title'].str.lower()\n",
    "regex_comp = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "stories.loc[:,'title']=stories['title'].apply((lambda row: regex_comp.sub('',row)))\n",
    "print('converted all title text to lower case and removed all punctuations')\n",
    "print('')\n",
    "print('5 rows for inspection')\n",
    "print('')\n",
    "print(stories.head())\n",
    "print('')\n",
    "stories['tokenized_title']=stories['title'].apply(lambda row: word_tokenize(row))\n",
    "print('tokenized title into individual words in a new column - tokenized title')\n",
    "print('')\n",
    "word_counts=pd.Series(name='word_counts')\n",
    "for tokenized_title in stories['tokenized_title']:\n",
    "    for word in tokenized_title:\n",
    "        if(re.match('\\w+',word)):\n",
    "            if(word not in word_counts):\n",
    "                word_counts[word]=1\n",
    "            else:\n",
    "                word_counts[word]+=1\n",
    "print('compiled series of distinct words and their counts in series word_counts')\n",
    "print('')\n",
    "print('first 5 rows of word_counts ')\n",
    "print('')\n",
    "print(word_counts.head(5))\n",
    "word_counts=word_counts[(word_counts>1)]\n",
    "print('removed words that occur only once')\n",
    "print('')\n",
    "df_stopwords=pd.DataFrame(stopwords.words('english'),columns=['word'])\n",
    "df_word_counts=pd.DataFrame({'word':word_counts.index,'counts':word_counts.values})\n",
    "joined_df=pd.merge(df_word_counts,df_stopwords,how='left',on='word',suffixes=('_wc', '_sw'),  indicator=True)\n",
    "joined_df=joined_df.loc[joined_df['_merge']=='left_only',:]\n",
    "print('removed stop words \\(default stopwords from nltk\\) from word_counts')\n",
    "print('')\n",
    "joined_df=joined_df.loc[joined_df['counts']>19,:]\n",
    "print('removed words that repeat less than 20 times')\n",
    "print('')\n",
    "no_of_words=len(joined_df)\n",
    "print('we now have {} words to use as features'.format(no_of_words))\n",
    "\n",
    "column_names=list(joined_df['word'].unique())\n",
    "counts_df=pd.DataFrame(0,index=np.arange(len(stories)),columns=column_names)\n",
    "print('created dataframe \\'counts_df\\' with the {} words/features as columns to implement bag of words model')\n",
    "print('')\n",
    "\n",
    "tokenized_titles=list(stories['tokenized_title'])\n",
    "for index,title_list in enumerate(tokenized_titles):\n",
    "    for word in title_list:\n",
    "        if(word in column_names):\n",
    "            counts_df[word].iloc[index]+=1\n",
    "print('first 5 rows of the \\'counts_df\\' dataframe for inspection' )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts_df, stories[\"points\"], test_size=0.2, random_state=1)\n",
    "print('split \\'counts_df\\' into 80% train and 20% test sets')\n",
    "print('')\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "print('trained linear regression model')\n",
    "print('')\n",
    "predictions=clf.predict(X_test)\n",
    "print('predicted upvotes for the test set')\n",
    "print('')\n",
    "diff=(y_test-predictions)**2\n",
    "mse=diff.sum(axis=0)/len(diff)\n",
    "print('Root mean error is:')\n",
    "print((mse)**(1/2))\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
